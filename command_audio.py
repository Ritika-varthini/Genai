# -*- coding: utf-8 -*-
"""command_audio

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZrvMVWDtNXIHS_ELf95eWcM8w7s6AmuM
"""

# Install necessary package
!pip install librosa gradio

# Import libraries
import os
import zipfile
import numpy as np
import librosa
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report
from google.colab import drive
# Mount Drive
drive.mount('/content/drive')

# Extract your zip dataset
zip_path = "/content/drive/MyDrive/sound/Audio(enAI).zip"
extract_path = "/content/extracted_audio"

if not os.path.exists(extract_path):
    os.makedirs(extract_path)

with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_path)

print(f"✅ Zip file extracted to: {extract_path}")
def extract_features(file_path):
    try:
        audio, sample_rate = librosa.load(file_path, duration=3, offset=0.5)
        mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)
        return mfccs
    except Exception as e:
        print(f"Error with {file_path}: {e}")
        return None

data_path = extract_path
labels = []
features = []

for label in os.listdir(data_path):
    label_folder = os.path.join(data_path, label)
    if not os.path.isdir(label_folder):
        continue
    for file in os.listdir(label_folder):
        if file.endswith(".wav"):
            file_path = os.path.join(label_folder, file)
            mfcc = extract_features(file_path)
            if mfcc is not None:
                features.append(mfcc)
                labels.append(label)

print("✅ Samples collected:", len(features))
if features:
    max_len = max([x.shape[1] for x in features])
    X = np.array([np.pad(x, ((0, 0), (0, max_len - x.shape[1])), mode='constant') for x in features])
    X = X[..., np.newaxis]

    encoder = LabelEncoder()
    y = encoder.fit_transform(labels)
    y = tf.keras.utils.to_categorical(y)

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    model = Sequential([
        Conv2D(32, (3,3), activation='relu', input_shape=X.shape[1:]),
        MaxPooling2D((2,2)),
        Dropout(0.3),
        Conv2D(64, (3,3), activation='relu'),
        MaxPooling2D((2,2)),
        Dropout(0.3),
        Flatten(),
        Dense(128, activation='relu'),
        Dropout(0.3),
        Dense(y.shape[1], activation='softmax')
    ])

    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    model.summary()

    history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))

    loss, accuracy = model.evaluate(X_test, y_test)
    print("✅ Test Accuracy: {:.2f}%".format(accuracy * 100))

    y_pred = model.predict(X_test)
    y_pred_classes = np.argmax(y_pred, axis=1)
    y_true = np.argmax(y_test, axis=1)

    print("\nClassification Report:")
    print(classification_report(y_true, y_pred_classes, target_names=encoder.classes_, labels=np.unique(y_true)))

    model.save("/content/speech_command_model.h5")
    print("✅ Model saved as speech_command_model.h5")

    # Save encoder classes for Gradio
    class_names = list(encoder.classes_)
else:
    print("🚨 No features were collected.")

"""First, let's extract the contents of the zip file."""

import gradio as gr
import numpy as np
import librosa
import tensorflow as tf
from sklearn.preprocessing import LabelEncoder
import os

# Load the trained model
model = tf.keras.models.load_model("/content/speech_command_model.h5")

# Re-initialize the LabelEncoder and fit it with the original labels to get class names
# Assuming the original labels can be reconstructed from the directory structure
extract_path = "/content/extracted_audio"
labels = []
for label in os.listdir(extract_path):
    label_folder = os.path.join(extract_path, label)
    if os.path.isdir(label_folder):
        for file in os.listdir(label_folder):
            if file.endswith(".wav"):
                labels.append(label)

encoder = LabelEncoder()
encoder.fit(labels)
class_names = list(encoder.classes_)

# Access the max_len determined during training from the previous cell
# This assumes that the previous cell (rSvM7SZb7WVN) has been successfully executed
# and the 'max_len' variable is available in the global scope.
# If not, you might need to re-calculate or load it.
try:
    max_len_training = max_len
except NameError:
    print("Error: max_len variable not found from training cell. Please ensure the training cell was executed successfully.")
    # You might want to handle this error more robustly, e.g., exit or raise an exception
    max_len_training = 130 # Fallback value - use with caution if training max_len is different


# Feature extraction for new input
def extract_mfcc(file_path, max_len_val):
    try:
        audio, sr = librosa.load(file_path, duration=3, offset=0.5)
        mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=40)

        if mfcc.shape[1] < max_len_val:
            # Pad with zeros if shorter
            pad_width = max_len_val - mfcc.shape[1]
            mfcc = np.pad(mfcc, ((0, 0), (0, pad_width)), mode='constant')
        else:
            # Truncate if longer
            mfcc = mfcc[:, :max_len_val]

        # Reshape for CNN input (batch_size, height, width, channels)
        mfcc = mfcc[np.newaxis, ..., np.newaxis]
        return mfcc
    except Exception as e:
        print(f"Error extracting features: {e}")
        return None


# Prediction function
def predict_command(audio_file):
    if audio_file is None:
        return "Please upload an audio file."

    # Pass the max_len_training to the feature extraction function
    mfcc = extract_mfcc(audio_file, max_len_training)
    if mfcc is None:
        return "Error processing audio file."

    prediction = model.predict(mfcc)
    predicted_index = np.argmax(prediction)
    predicted_label = class_names[predicted_index]
    confidence = prediction[0][predicted_index]
    return f"🗣️ Predicted: {predicted_label} ({confidence*100:.2f}%)"

# Gradio Interface
interface = gr.Interface(
    fn=predict_command,
    inputs=gr.Audio(type="filepath", label="Upload a .wav command"),
    outputs=gr.Text(label="Prediction"),
    title="🎤 Speech Command Recognition",
    description="Upload a short .wav file (1–3 seconds) to identify the speech command"
)

interface.launch()

"""Now, let's update the `data_path` in your original code to point to the extracted directory."""

